Questions

Please write a short response (4-5 sentences / short paragraph) to
each of the following questions / the following question. Your
responses will be graded for accuracy, critical thinking, and
clarity. You may use any common word processing or text format. Please
upload your answers by the due date.

Question 1: In the readings we see an old version of a positional
encoding (sine function) and an even older version of word embeddings
(word2vec). What are the pros and cons of learning these embeddings
directly as opposed to a pre-defined/pre-learned embedding as
described in the readings?

Positional encodings can use either a pre-defined or learned encoding,
and these each have their pros and cons. Pre-defined encodings are
really nice because you can extrapolate to longer sequences than
you've seen in training, don't require additional parameters to train,
and are much faster. Learned encodings have none of those advantages,
but unlike positional encodings they can learn significant positional
relationships in a particular dataset, so in situations where you have
substantial data they can sometimes perform even better than
pre-defined ones.

Using existing pretrained word embeddings like word2vec is a similar
story. Word2vec is computationally simple and has nice properties like
well-behaved vectors (e.g. king - man + woman = queen), but it cannot
be used for new words in your vocabulary, and is trained with just
co-occurance via continuous bag of words. In comparison, learning an
embedding table for your entire vocabulary can incur substantial
computational cost, but with enough data you can learn more complex
relationships between words compared to the naive continuous bag of
words approach used in word2vec.
