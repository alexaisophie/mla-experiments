#+TITLE: On MLA

* On Multi-Head Latent Attention

Multi-head Latent Attention (MLA) is a variant of multi-head attention which was introduced in the DeepSeek-V2 paper. There are several variants of multi-head attention whose purpose is primarily to reduce the KV-cache size, which is a memory bottleneck that emerges from scaling large models. These methods, which include Group-Query Attention and Multi-Query Attention, are primarily considered /performance tradeoffs/, i.e. the performance is worse, but you get to scale them much further by reducing the memory overhead.

In comparison, MLA accomplishes this by using a low-rank factorized projection matrix, operating a bit like multi-query attention where the decompression process yields the unique, appropriate KV for that particular head. DeepSeek claims this not only helps the memory overhead, but also /improves/ the model rather than suffering for its inclusion. The basic idea is as follows:

1. Replace the QKV computation by using low rank factorization to turn one matrix of dim [in, out] to two matrices of [in, lora_rank] and [lora_rank, out].
2. Project the compressed KV latent vector for each head to get the full K and V head corresponding to each Q head.
3. Cache the compressed latent KV vector instead of each of the KV heads, and compute the KV heads on the fly from the latent vector.

There's also an element of this which makes this compatible with RoPE, but for simplicity we will start by just exploring what the above part accomplishes differently from normal MHA.

** Why Low-Rank Factorization

Using two matrices instead of one, in theory, has some tradeoffs. The practice of factorizing a matrix (in this case the Q and KV matrices) into a product of matrices (in this case the compression and decompression matrices) is called /matrix decomposition/. In the case here, where we are decomposing a matrix (a, b) into two matrices (a, r), (r, b), is called /rank factorization/. You can pretty much always do this, but the smaller the value of /r/ the more likely you will lose substantial data about your original matrix.

When using low-rank factorization for layers in a neural network, the good things are that the compressed matrices use fewer parameters ((1536(5120 + 24576) < 5120 * 24576)), (4.5e7 vs 1.2e8)), and are somewhat more expressive (by virtue of having two layers sometimes separated by a nonlinearity like a layernorm or activation. They can end up roughly approximate or equivalent to one larger matrix, so in theory you could multiply the weights of these matrices together to "retrieve your original matrix", albeit with some compression loss.

The weaknesses of doing this are that we now have to perform 2 operations for every time we use this (i.e. we double the total matmuls for each layer we compress and decompress, in exchange for making them small), and will obviously lose some of the original representation of the original matrix. You can think of low-rank decomposition as trading memory costs for computation costs -- they are popular for things like finetuning and here for kv-cache reduction, since those things are gated by total memory cost rather than computational overhead or inference speed. 

In our case, we are hoping that we can preserve the functionality of W_qkv as much as possible while reducing the size of the cache by as much as we can. This should, in theory, allow us to enjoy the performance of normal multi-head attention while keeping the kv cache overhead minimal.

** TODO Related Work

https://github.com/madsys-dev/deepseekv2-profile/blob/924174cb5dc11fad24bdaad3fd820ebf87506368/workspace/blog/optimizing-mla.md

https://arxiv.org/abs/2405.04434

** TODO Experiments

mqa experiments

MLA has two components which make it different from standard multi-head attention. First, it has compression and uncompression matrices. Second, it has additional uncompression matrices whose purpose is to carry RoPE position embeddings, which is abnormal practice for RoPE embeddings which are normally applied to the full attention vector.

It's not immediately clear which of these practices contribute to MLA's viability. For example, it's possible the low-rank factorization adds a lot, and the RoPE extension salvages some of the lost capability from the effectiveness of RoPE.  It's also possible that a dedicated part of the vector whose purpose is to carry RoPE information is the more beneficial component, and the low-rank steps are a marginal benefit. Likewise, it's also possible both of these by themselves are insufficient to see substantial gains, but together they synergize to perform well. Finally, it's possible that MLA is not inherently superior to MHA at all, but the ability to reduce the burden upon the KV cache makes it worthwhile to use anyways.

To investigate this, we implement a variant of Multi-head Latent Attention which does not include RoPE. We instead use standard position encodings, and compare this to vanilla Multi-Head Attention using standard position encoding. This way, we can decouple the pros and cons of the RoPE components of MLA, as an ablation study. 

*** Experiment 1a

Because the low-rank decomposition reduces the total number of parameters in the network, we do two experiments to compare MLA's relative efficacy compared to vanilla multi-headed attention. 

First, we train two models exactly equivalent except replacing MHA with MLA in one of the networks. This knocks our 35M parameter model to 31M parameters. We train both models for 1 epoch on about 100M tokens.

The MHA model reaches a slightly lower loss, but the models are fairly comparable. 

KV cache usage, and performance of both models as a comparison.

*** Experiment 1b

In our second experiment, we add an additional layer to the MLA model, which brings it back up to 34.6M parameters. It's still a bit smaller than the 35M model, but it's close enough to serve as a point of comparison for equal parameter counts with and without MLA.

This performs a little better than the model from experiment 1, and comes just shy of the original MHA network with 35M parameters..

*** Experiment 2a

We also want to test inference speed with the new KV caching method, and how the additional matmuls affect the throughput. For this experiment, we use a fixed prompt of 100 tokens and measure the time to generate between 20 and 100 tokens, to observe how the token count affects the speed of autoregressive output.

*** Experiment 2b

Likewise, we invert the previous experiment and use a variable prompt of between 20 and 100 tokens and measure the time to generate 100 tokens. This is largely identical stratified by model (i.e. a single model will always be faster than another model, and the latency values of both models are about the same in both cases no matter how long the input prompt is

*** Other Experiments

 Add RoPE stuff if you have time, add MQA comparison

** TODO Results

TODO: Multi-query attention baseline

There are three primary advantages to using MLA over normal multi-head attention. 

1. Reduced parameter count
2. Reduced KV Cache size
3. Greater expressivity w/ same parameter count and more layers

| Model Description | Training Perplexity | KV Cache Parameters per Token |
|-------------------+---------------------+-------------------------------|
| MHA 35M           | 59.25               |                          8192 |
| MLA 34.6M         | 64.27               |                          1152 |
| MLA 31M           | 65.12               |                          1024 |
| MQA Baseline      | ??                  |                            ?? |

Experiment 1 focuses on the first of these -- We use an explicitly smaller model, and we get comparable (but slightly lower) performance from it.

Experiment 2 focuses on the third of these -- We use an explicitly "larger" model with roughly the same number of parameters. In this case, we spend our advantage from saved parameters on more layers. We still get lower performance here compared to full MHA, but it is a bit better than the model from experiment 1.

[[./figures/reference_training_curve.png]]

[[./figures/training_curve_31M.png]]

[[./figures/training_curve_34M.png]]

*** Inference Time Experiments

Contrary to what they describe in the DeepSeek-V2 paper, the modeling code for the open-sourced DeepSeek-V2 weights just uses regular full KV caching, rather than compressing KV and caching that. 

This is because it's slower if you have to do the decompression layer to retrieve KV from compressed KV, and if you have extra space, it's faster to just store those values directly.  it takes more memory, so it's really important if you want to do batched inference and serve to customers. It's also important to recognize that these operations are (roughly) equivalent -- the only major difference is that we cache earlier or later along the inference logic flow, not that we are ending up with substantially different values one way or the other.

You may ask: how different is the performance between compressed caching and full KV caching? We will implement two versions of ropeless MLA to see how much different it is: one using a compressed KV cache and one using the standard full KV cache similar to their open source modeling code. 

[[./figures/inference_100_in.png]]

[[./figures/inference_100_out.png]]

The above plots follow fairly nicely from the architectures they represent. The reference MHA implementation with full KV caching is faster than all the other models, since it performs fewer matmuls (due to not doing compression -> decompression operations). The 31M model is faster than the 34.6M model, and for both models full KV caching is faster than compressed KV caching (due to using fewer matmuls to uncompress K and V).

In all cases, we substantially see improved autoregressive generation time compared to not using a KV cache, and in the compressed KV case we see the memory requirements slashed a very large amount. 

** Discussion

In both cases, the network performed admirably. Likewise, in both cases, we substantially address the KV cache problem of scaling the model to very large sizes -- that the KV cache burden can be reduced substantially with minimal loss in performance is significant. In addition, the memory saved will grow with the size of the network: whereas Multi-Head Attention uses (2 * n_heads * d_heads * layers) KV cache per token, in comparison the ropeless MLA uses (d_compression * layer) or roughly (4 * d_heads * layers). At large model size, this is extremely, extremely large. [madsys-dev](https://github.com/madsys-dev/deepseekv2-profile/blob/924174cb5dc11fad24bdaad3fd820ebf87506368/workspace/blog/optimizing-mla.md) with a larger model saw a reduction from 81.92 kB cache per token to 1.15 kB per token, a reduction of 98.6% in size. To bring the KV cache from a major architectural bottleneck to a relative non-issue is certainly extremely noteworthy. 

However, it does seem to have lower throughput compared to normal attention -- both the additional layer and the addition of two matrices to replace one in every attention block adds some relatively significant cost to inference time. You can use full KV caching to speed this up, but then you lose out on the largest benefit of the architecture in the first place (very small KV cache), while still remaining slower than regular MHA. It's important to point this out relative to variants like multi-query attention, which are worse-performing and use larger caches than MLA, but are expressly /faster/ than regular multi-head attention. 

Overall, we do not see the claimed equivalent-or-superior performance claimed in the DeepSeek-v2 paper with this ropeless variant of MLA. This could be for a variety of reasons:

1. The RoPE component of MLA, beyond being a simple hack, is what elevates the performance of MLA to at-or-above MHA
2. The superior performance of MLA emerges at larger scales, where both the model and the input sequences are much larger.
3. MHA and MLA could perform more comparably when dealing with actual measured capabilities, compared to training data perplexity -- it could be that MLA's higher perplexity represents a resistance to overfitting beyond being a genuinely useful metric for measuring performance.
4. MLA being more compatible with modern training paradigms like mixed-precision, etc. 

Future work here could include:
- Extending RopelessMLA to use RoPE
- using a much larger model on a bigger dataset to measure capability directly
- Optimizing the implementation further beyond this toy pedagogical implementation

MLA has demonstrated it's usefulness here in scaling up very large models. However, it remains to be seen if the claimed benefits extend to smaller models.
