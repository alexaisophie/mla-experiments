#+TITLE: On MLA

* On Multi-Head Latent Attention

https://github.com/madsys-dev/deepseekv2-profile/blob/924174cb5dc11fad24bdaad3fd820ebf87506368/workspace/blog/optimizing-mla.md

https://arxiv.org/abs/2405.04434

Multi-head Latent Attention (MLA) is a variant of multi-head attention which was introduced in the DeepSeek-V2 paper. There are several variants of multi-head attention whose purpose is primarily to reduce the KV-cache size, which is a memory bottleneck that emerges from scaling large models. These methods, which include Group-Query Attention and Multi-Query Attention, are primarily considered /performance tradeoffs/, i.e. the performance is worse, but you get to scale them by reducing the memory overhead.

In comparison, MLA accomplishes this by using a low-rank factorized projection matrix, operating a bit like multi-query attention where the decompression process yields the unique, appropriate KV for that particular head. DeepSeek claims this not only helps the memory overhead, but also /improves/ the model rather than suffering for its inclusion. The basic idea is as follows:

1. Replace the QKV computation by using low rank factorization to turn one matrix of dim [in, out] to two matrices of [in, lora_rank] and [lora_rank, out].
2. Project the compressed KV latent vector for each head to get the full K and V head corresponding to each Q head.
3. Cache the compressed latent KV vector instead of each of the KV heads, and compute the KV heads on the fly from the latent vector.

There's also an element of this which makes this compatible with RoPE, but for simplicity we will start by just exploring what the above part accomplishes differently from normal MHA.

** Why Low-Rank Factorization

Using two matrices instead of one, in theory, has some tradeoffs.

Fewer params (1536(5120 + 24576) < 5120 * 24576)) (4.5e7 vs 1.2e8)

more expressive

more matmuls, more memory on activations (?)

two step linear projection?

you can multiply the matrices together and "get your original matrix", but not compressed and with lower expressive power. This will give you back your original MHA, so this part is just compressing the operation and adding additional capacity for nonlinearity in exchange for lowering the rank.

low-rank factorization can capture more linearities

** Experiments

MLA has two components which make it different from standard multi-head attention. First, it has compression and uncompression matrices. Second, it has additional uncompression matrices whose purpose is to carry RoPE position embeddings, which is abnormal practice for RoPE embeddings which are normally applied to the full attention vector.

It's not immediately clear which of these practices contribute to MLA's viability. For example, it's possible the low-rank factorization adds a lot, and the RoPE extension salvages some of the lost capability from the effectiveness of RoPE.  It's also possible that a dedicated part of the vector whose purpose is to carry RoPE information is the more beneficial component, and the low-rank steps are a marginal benefit. Likewise, it's also possible both of these by themselves are insufficient to see substantial gains, but together they synergize to perform well.

To investigate this, we implement a variant of Multi-head Latent Attention which does not include RoPE. We instead use standard position encodings, and compare this to vanilla Multi-Head Attention using standard position encoding. This way, we can decouple the pros and cons of the RoPE components of MLA, as an ablation study. 

We train two models of the same size, all exactly equivalent except replacing MHA with MLA in one of the networks. We report parameter sizes, KV cache usage, and performance of both models as a comparison.

Add RoPE stuff if you have time

** Results

** Discussion

