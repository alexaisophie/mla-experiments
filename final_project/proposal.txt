# Proposal

Multi-head Latent Attention (MLA) is a variant of multi-head attention which was introduced in the DeepSeek-V2 paper. There are several variants of multi-head attention whose purpose is primarily to reduce the KV-cache size, which is a memory bottleneck that emerges from scaling large models. These methods, which include Group-Query Attention and Multi-Query Attention, are primarily considered /performance tradeoffs/, i.e. the performance is worse, but you get to scale them much further by reducing the memory overhead.

In comparison, MLA accomplishes this by using a low-rank factorized projection matrix, operating a bit like multi-query attention where instead of repeating a single head several times, you decompress a latent vector to yield a unique, appropriate KV for each particular head. DeepSeek claims this not only helps the memory overhead, but also /improves/ the model rather than suffering for its inclusion. The basic idea is as follows:

1. Replace the QKV computation by using low rank factorization to turn one matrix of dim [in, out] to two matrices of [in, lora_rank] and [lora_rank, out].
2. Project the compressed KV latent vector for each head to get the full K and V head corresponding to each Q head.
3. Cache the compressed latent KV vector instead of each of the KV heads, and compute the KV heads on the fly from the latent vector.

There is also a component of this which introduces /decoupled RoPE/, making certain attention heads explicitly responsible for the position encoding for the entire block. For simplicity's sake, we start with a version which only uses the low-rank factorization, and then add it back in later.

## Planned Experiments / Work Already Completed

[x] means completed, [ ] means yet to be completed

Experiment 1: Compare MLA to Multi-Head Attention and Multi-Query Attention, without RoPE

Capture information about KV Cache size + training perplexity

- [x] all experiments without RoPE on 30M models
- [x] all experiments without RoPE on 300M models

Experiment 2: Compare MLA to Multi-Head Attention and Multi-Query Attention, with RoPE

Capture information about KV Cache size + training perplexity

- [x] MHA with RoPE 30M
- [x] MQA with RoPE 30M
- [ ] MLA with RoPE 30M (need to implement)
- [ ] MHA with RoPE 300M (just need to run)
- [ ] MQA with RoPE 300M (just need to run)
- [ ] MLA with RoPE 300M (just need to run)

Experiment 3: Compare different attention mechanisms for inference speed

- [x] MHA
- [x] MQA
- [x] MLA with compressed KV caching
- [x] MLA with full KV caching
